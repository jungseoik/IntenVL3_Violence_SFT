import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def load_and_preprocess_data(csv_file_path):
    """
    CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        csv_file_path (str): CSV íŒŒì¼ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(csv_file_path)
    
    print("=== ë°ì´í„° ê¸°ë³¸ ì •ë³´ ===")
    print(f"ì´ ë°ì´í„° ìˆ˜: {len(df)}")
    print(f"ê²°ì¸¡ê°’ í˜„í™©:")
    print(df.isnull().sum())
    print()
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬ (predicted_categoryê°€ nullì¸ ê²½ìš° ì œì™¸)
    df_clean = df.dropna(subset=['predicted_category']).copy()
    print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° ìˆ˜: {len(df_clean)}")
    
    return df_clean

def normalize_categories_for_binary_classification(df):
    """
    ì´ì§„ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¹´í…Œê³ ë¦¬ ì •ê·œí™” (Normal vs Abnormal).
    Normalì´ ì•„ë‹ˆë©´ ëª¨ë‘ Abnormalë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    
    Returns:
        pd.DataFrame: ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„
    """
    df = df.copy()
    
    # ëŒ€ì†Œë¬¸ì í†µì¼ ë° ê³µë°± ì œê±°
    df['ground_truth_clean'] = df['ground_truth'].str.lower().str.strip()
    df['predicted_category_clean'] = df['predicted_category'].str.lower().str.strip()
    
    # ì´ì§„ë¶„ë¥˜: Normal vs Abnormal ë³€í™˜
    df['ground_truth_binary'] = df['ground_truth_clean'].apply(
        lambda x: 'normal' if x == 'normal' else 'abnormal'
    )
    
    df['predicted_category_binary'] = df['predicted_category_clean'].apply(
        lambda x: 'normal' if x == 'normal' else 'abnormal'
    )
    
    print("=== ì´ì§„ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ê·œí™” ê²°ê³¼ ===")
    print("Ground Truth ë¶„í¬:")
    print(df['ground_truth_binary'].value_counts())
    print("\nPredicted ë¶„í¬:")
    print(df['predicted_category_binary'].value_counts())
    print()
    
    # ì›ë³¸ ì¹´í…Œê³ ë¦¬ë³„ ë³€í™˜ ê²°ê³¼ í™•ì¸
    print("=== ì›ë³¸ â†’ ì´ì§„ë¶„ë¥˜ ë³€í™˜ ë§¤í•‘ ===")
    print("Ground Truth ë³€í™˜:")
    gt_mapping = df.groupby(['ground_truth_clean', 'ground_truth_binary']).size().reset_index()
    gt_mapping.columns = ['original', 'binary', 'count']
    for _, row in gt_mapping.iterrows():
        print(f"  {row['original']} â†’ {row['binary']} ({row['count']}ê°œ)")
    
    print("\nPredicted ë³€í™˜ (ìƒìœ„ 10ê°œ):")
    pred_mapping = df.groupby(['predicted_category_clean', 'predicted_category_binary']).size().reset_index()
    pred_mapping.columns = ['original', 'binary', 'count']
    pred_mapping = pred_mapping.sort_values('count', ascending=False).head(10)
    for _, row in pred_mapping.iterrows():
        print(f"  '{row['original']}' â†’ {row['binary']} ({row['count']}ê°œ)")
    print()
    
    return df

def calculate_accuracy_by_case(df):
    """
    ê° ì¼€ì´ìŠ¤ë³„ë¡œ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„
    
    Returns:
        pd.DataFrame: ì¼€ì´ìŠ¤ë³„ ì •í™•ë„ ê²°ê³¼
    """
    # ì •ë‹µ ì—¬ë¶€ ê³„ì‚° (ì´ì§„ë¶„ë¥˜ ê¸°ì¤€)
    df['is_correct'] = (df['ground_truth_binary'] == df['predicted_category_binary'])
    
    # ê° ì¼€ì´ìŠ¤ë³„ ê·¸ë£¹í™”
    case_results = df.groupby(['template_type', 'num_segment']).agg({
        'is_correct': ['count', 'sum', 'mean'],
        'video_name': 'nunique'
    }).round(4)
    
    # ì»¬ëŸ¼ëª… ì •ë¦¬
    case_results.columns = ['total_count', 'correct_count', 'accuracy', 'unique_videos']
    case_results = case_results.reset_index()
    
    # ì •í™•ë„ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
    case_results['accuracy_percent'] = (case_results['accuracy'] * 100).round(2)
    
    # ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
    for idx, row in case_results.iterrows():
        template = row['template_type']
        segment = row['num_segment']
        subset = df[(df['template_type'] == template) & (df['num_segment'] == segment)]
        
        # True Positive, False Positive, True Negative, False Negative ê³„ì‚°
        tp = len(subset[(subset['ground_truth_binary'] == 'abnormal') & (subset['predicted_category_binary'] == 'abnormal')])
        fp = len(subset[(subset['ground_truth_binary'] == 'normal') & (subset['predicted_category_binary'] == 'abnormal')])
        tn = len(subset[(subset['ground_truth_binary'] == 'normal') & (subset['predicted_category_binary'] == 'normal')])
        fn = len(subset[(subset['ground_truth_binary'] == 'abnormal') & (subset['predicted_category_binary'] == 'normal')])
        
        case_results.loc[idx, 'true_positive'] = tp
        case_results.loc[idx, 'false_positive'] = fp
        case_results.loc[idx, 'true_negative'] = tn
        case_results.loc[idx, 'false_negative'] = fn
        
        # Precision, Recall, F1-score ê³„ì‚°
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        case_results.loc[idx, 'precision'] = round(precision, 4)
        case_results.loc[idx, 'recall'] = round(recall, 4)
        case_results.loc[idx, 'f1_score'] = round(f1, 4)
    
    return case_results

def detailed_performance_analysis(df):
    """
    ìƒì„¸í•œ ì„±ëŠ¥ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = {}
    
    # ì „ì²´ ì„±ëŠ¥
    overall_accuracy = (df['ground_truth_binary'] == df['predicted_category_binary']).mean()
    results['overall_accuracy'] = overall_accuracy
    
    # í…œí”Œë¦¿ë³„ ì„±ëŠ¥
    template_performance = df.groupby('template_type').apply(
        lambda x: (x['ground_truth_binary'] == x['predicted_category_binary']).mean()
    )
    results['template_performance'] = template_performance
    
    # num_segmentë³„ ì„±ëŠ¥
    segment_performance = df.groupby('num_segment').apply(
        lambda x: (x['ground_truth_binary'] == x['predicted_category_binary']).mean()
    )
    results['segment_performance'] = segment_performance
    
    return results



def analyze_error_patterns(df):
    """
    ì˜¤ë‹µ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„
    
    Returns:
        pd.DataFrame: ì˜¤ë‹µ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
    """
    # ì˜¤ë‹µì¸ ê²½ìš°ë§Œ í•„í„°ë§ (ì´ì§„ë¶„ë¥˜ ê¸°ì¤€)
    errors = df[df['ground_truth_binary'] != df['predicted_category_binary']].copy()
    
    if len(errors) == 0:
        print("ì˜¤ë‹µì´ ì—†ìŠµë‹ˆë‹¤!")
        return pd.DataFrame()
    
    # ì´ì§„ë¶„ë¥˜ ì˜¤ë‹µ íŒ¨í„´ ë¶„ì„
    print("=== ì´ì§„ë¶„ë¥˜ ì˜¤ë‹µ íŒ¨í„´ ===")
    binary_error_patterns = errors.groupby(['ground_truth_binary', 'predicted_category_binary']).size().reset_index()
    binary_error_patterns.columns = ['ground_truth', 'predicted', 'error_count']
    binary_error_patterns = binary_error_patterns.sort_values('error_count', ascending=False)
    print(binary_error_patterns.to_string(index=False))
    print()
    
    # ì›ë³¸ ì¹´í…Œê³ ë¦¬ ì˜¤ë‹µ íŒ¨í„´ë„ í™•ì¸
    print("=== ì›ë³¸ ì¹´í…Œê³ ë¦¬ ì˜¤ë‹µ íŒ¨í„´ (ìƒìœ„ 10ê°œ) ===")
    original_error_patterns = errors.groupby(['ground_truth_clean', 'predicted_category_clean']).size().reset_index()
    original_error_patterns.columns = ['ground_truth_original', 'predicted_original', 'error_count']
    original_error_patterns = original_error_patterns.sort_values('error_count', ascending=False).head(10)
    print(original_error_patterns.to_string(index=False))
    print()
    
    # ì¼€ì´ìŠ¤ë³„ ì˜¤ë‹µë¥ 
    case_error_rates = errors.groupby(['template_type', 'num_segment']).size().reset_index()
    case_error_rates.columns = ['template_type', 'num_segment', 'error_count']
    
    # ì „ì²´ ì¼€ì´ìŠ¤ë³„ ë°ì´í„° ìˆ˜ì™€ ë³‘í•©
    case_totals = df.groupby(['template_type', 'num_segment']).size().reset_index()
    case_totals.columns = ['template_type', 'num_segment', 'total_count']
    
    case_analysis = pd.merge(case_totals, case_error_rates, on=['template_type', 'num_segment'], how='left')
    case_analysis['error_count'] = case_analysis['error_count'].fillna(0)
    case_analysis['error_rate'] = (case_analysis['error_count'] / case_analysis['total_count'] * 100).round(2)
    
    return case_analysis

def generate_comprehensive_report(csv_file_path):
    """
    ì¢…í•©ì ì¸ í‰ê°€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        csv_file_path (str): CSV íŒŒì¼ ê²½ë¡œ
    """
    print("ğŸ” ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df = load_and_preprocess_data(csv_file_path)
    df = normalize_categories_for_binary_classification(df)
    
    # 2. ì¼€ì´ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
    print("ğŸ“Š ì¼€ì´ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ (ì´ì§„ë¶„ë¥˜)")
    print("=" * 60)
    case_results = calculate_accuracy_by_case(df)
    
    # ì£¼ìš” ë©”íŠ¸ë¦­ë§Œ ì¶œë ¥
    display_columns = ['template_type', 'num_segment', 'accuracy_percent', 'precision', 'recall', 'f1_score', 
                      'true_positive', 'false_positive', 'true_negative', 'false_negative']
    print(case_results[display_columns].to_string(index=False))
    print()
    
    # 3. ìƒì„¸ ì„±ëŠ¥ ë¶„ì„
    print("ğŸ“ˆ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„")
    print("=" * 60)
    detailed_results = detailed_performance_analysis(df)
    
    print(f"ì „ì²´ ì •í™•ë„: {detailed_results['overall_accuracy']:.4f} ({detailed_results['overall_accuracy']*100:.2f}%)")
    print()
    
    print("í…œí”Œë¦¿ë³„ ì„±ëŠ¥:")
    for template, accuracy in detailed_results['template_performance'].items():
        print(f"  {template}: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print()
    
    print("ì„¸ê·¸ë¨¼íŠ¸ë³„ ì„±ëŠ¥:")
    for segment, accuracy in detailed_results['segment_performance'].items():
        print(f"  {segment} segments: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print()
    
    # 4. ì˜¤ë‹µ íŒ¨í„´ ë¶„ì„
    print("âŒ ì˜¤ë‹µ íŒ¨í„´ ë¶„ì„")
    print("=" * 60)
    error_analysis = analyze_error_patterns(df)
    if not error_analysis.empty:
        print("ì¼€ì´ìŠ¤ë³„ ì˜¤ë‹µ í˜„í™©:")
        print(error_analysis.to_string(index=False))
    print()
    
    # 5. ìµœê³  ì„±ëŠ¥ ì¼€ì´ìŠ¤ ì°¾ê¸°
    print("ğŸ† ìµœê³  ì„±ëŠ¥ ì¼€ì´ìŠ¤")
    print("=" * 60)
    best_case = case_results.loc[case_results['accuracy'].idxmax()]
    print(f"ìµœê³  ì„±ëŠ¥: {best_case['template_type']} + {best_case['num_segment']} segments")
    print(f"ì •í™•ë„: {best_case['accuracy']:.4f} ({best_case['accuracy_percent']:.2f}%)")
    print(f"ì •ë‹µ ìˆ˜: {best_case['correct_count']}/{best_case['total_count']}")
    print()
    
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    
    return df, case_results


if __name__ == "__main__":
    csv_file_path = "results.csv"  # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”
    df, case_results = generate_comprehensive_report(csv_file_path)
